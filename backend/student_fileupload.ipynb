{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb5bf9d5",
   "metadata": {},
   "source": [
    "IMPORTS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a2035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import fitz\n",
    "from docx import Document\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4a204",
   "metadata": {},
   "source": [
    "DB CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c67916",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "client = MongoClient(\"mongodb+srv://daktrboys05_db_user:gdgclubproject@to-do-list.qmqixqe.mongodb.net/\")\n",
    "db = client[\"tries_db\"]\n",
    "\n",
    "questions_collection = db[\"questions\"]\n",
    "submissions_collection = db[\"submissions\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafdf8cd",
   "metadata": {},
   "source": [
    "REQUEST BODY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217bcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateRequest(BaseModel):\n",
    "    submission_id: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cdb9fb",
   "metadata": {},
   "source": [
    "EMBEDDING MANAGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf38c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embeddingManager:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        return self.model.encode(texts)\n",
    "embedding_manager = embeddingManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4e0f5",
   "metadata": {},
   "source": [
    "FETCH SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3de8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_submission(submission_id: str):\n",
    "    return submissions_collection.find_one(\n",
    "        {\"_id\": submission_id},\n",
    "        {\"answer_file_url\": 1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9ccf2",
   "metadata": {},
   "source": [
    "DOWNLOADING FILE FORM CLOUDINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e07c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_cloudinary(file_url: str) -> str:\n",
    "    response = requests.get(file_url, stream=True, timeout=30)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    ext = os.path.splitext(file_url.split(\"?\")[0])[1]\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:\n",
    "        for chunk in response.iter_content(8192):\n",
    "            tmp.write(chunk)\n",
    "        return tmp.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63210f",
   "metadata": {},
   "source": [
    "EXTRACTING DOCX FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c59a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    collected = []\n",
    "\n",
    "    # Body paragraphs\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if text:\n",
    "            collected.append(text)\n",
    "\n",
    "    # Tables\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            for cell in row.cells:\n",
    "                cell_text = cell.text.strip()\n",
    "                if cell_text:\n",
    "                    collected.append(cell_text)\n",
    "\n",
    "    # Headers & footers\n",
    "    for section in doc.sections:\n",
    "        header = section.header\n",
    "        footer = section.footer\n",
    "\n",
    "        for para in header.paragraphs:\n",
    "            if para.text.strip():\n",
    "                collected.append(para.text.strip())\n",
    "\n",
    "        for para in footer.paragraphs:\n",
    "            if para.text.strip():\n",
    "                collected.append(para.text.strip())\n",
    "\n",
    "    # Embedded images ‚Üí OCR\n",
    "    for rel in doc.part.rels.values():\n",
    "        if \"image\" in rel.target_ref:\n",
    "            image_bytes = rel.target_part.blob\n",
    "            img = Image.open(io.BytesIO(image_bytes))\n",
    "            ocr_text = pytesseract.image_to_string(\n",
    "                img,\n",
    "                lang=\"eng\",\n",
    "                config=\"--psm 6\"\n",
    "            )\n",
    "            if ocr_text.strip():\n",
    "                collected.append(ocr_text.strip())\n",
    "\n",
    "    return \"\\n\".join(collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32591c3",
   "metadata": {},
   "source": [
    "EXTRACTING PDF FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194563f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    texts = []\n",
    "\n",
    "    for page_num, page in enumerate(doc):\n",
    "        # Digital text\n",
    "        page_text = page.get_text().strip()\n",
    "        if page_text:\n",
    "            texts.append(page_text)\n",
    "\n",
    "        # OCR embedded images\n",
    "        images = page.get_images(full=True)\n",
    "        for img in images:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            img_pil = Image.open(io.BytesIO(image_bytes))\n",
    "            ocr_text = pytesseract.image_to_string(\n",
    "                img_pil,\n",
    "                lang=\"eng\",\n",
    "                config=\"--psm 6\"\n",
    "            ).strip()\n",
    "\n",
    "            if ocr_text:\n",
    "                texts.append(ocr_text)\n",
    "\n",
    "        # Full-page OCR fallback\n",
    "        if not page_text and not images:\n",
    "            pix = page.get_pixmap(dpi=300)\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            ocr_text = pytesseract.image_to_string(img, lang=\"eng\").strip()\n",
    "\n",
    "            if ocr_text:\n",
    "                texts.append(ocr_text)\n",
    "\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a6a8e",
   "metadata": {},
   "source": [
    "EXTRACT IMAGE UPLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ec6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    return pytesseract.image_to_string(img).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7cf20",
   "metadata": {},
   "source": [
    "SPLITTING FOR TEXT ANSWERS BY STUDENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bad246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_answers(text: str) -> list[str]:\n",
    "    pattern = r\"(Question\\s+\\d+[\\s\\S]*?)(?=Question\\s+\\d+|$)\"\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "    answer_blocks = []\n",
    "    for idx, block in enumerate(matches, start=1):\n",
    "        answer_blocks.append(\n",
    "            f\"[ANSWER_{idx}_START]\\n{block.strip()}\"\n",
    "        )\n",
    "\n",
    "    return answer_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e917c9",
   "metadata": {},
   "source": [
    "FUNCTION TO EXTRACT FILE TYPE FROM STUDENT ANSWERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f32e1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_student_text(answer_file_path):\n",
    "    ext = os.path.splitext(answer_file_path)[1].lower()\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        pdf_texts = extract_text_from_pdf(answer_file_path)\n",
    "        full_text = \"\\n\".join(pdf_texts)\n",
    "        return split_by_answers(full_text)\n",
    "\n",
    "    elif ext == \".docx\":\n",
    "        docx_texts = extract_text_from_docx(answer_file_path)\n",
    "        full_text = \"\\n\".join(docx_texts)\n",
    "        return split_by_answers(full_text)\n",
    "\n",
    "    elif ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "        img_texts = extract_text_from_image(answer_file_path)\n",
    "        full_text = \"\\n\".join(img_texts)\n",
    "        return split_by_answers(full_text)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9838de6",
   "metadata": {},
   "source": [
    "FETCH QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1455ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_question(exam_id: str, question_number: int):\n",
    "    return questions_collection.find_one(\n",
    "        {\n",
    "            \"exam_id\": exam_id,\n",
    "            \"question_number\": question_number\n",
    "        },\n",
    "        {\"_id\": 0}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf581ab",
   "metadata": {},
   "source": [
    "SEMANTIC FALLBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70f4f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_fallback(question, student_answer, embedding_manager) -> int:\n",
    "    \"\"\"\n",
    "    Fallback scoring using semantic similarity\n",
    "    when rubric config or LLM grading fails.\n",
    "    \"\"\"\n",
    "\n",
    "    if not student_answer.strip():\n",
    "        return 0\n",
    "\n",
    "    student_emb = embedding_manager.generate_embeddings([student_answer])\n",
    "    anchor_emb = embedding_manager.generate_embeddings([question[\"question_text\"]])\n",
    "\n",
    "    similarity = cosine_similarity(student_emb, anchor_emb)[0][0]\n",
    "\n",
    "    max_marks = question[\"max_marks\"]\n",
    "\n",
    "    # Convert similarity ‚Üí marks\n",
    "    score = int(similarity * max_marks)\n",
    "\n",
    "    # Safety floor so good answers never get 0\n",
    "    if score == 0:\n",
    "        score = max(1, int(0.3 * max_marks))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e537e5f",
   "metadata": {},
   "source": [
    "KEYWORD SIMILARITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "749b8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_score(student_answer: str, keywords: list[str]) -> float:\n",
    "    if not keywords:\n",
    "        return 0.0\n",
    "\n",
    "    text = student_answer.lower()\n",
    "    hits = sum(1 for kw in keywords if kw.lower() in text)\n",
    "    return hits / len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e5de6",
   "metadata": {},
   "source": [
    "ANSWER CHUNK SIMILARITY CHECKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "274b8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_chunk_score(\n",
    "    student_answer: str,\n",
    "    solution_chunks: list[str],\n",
    "    embedding_manager\n",
    ") -> float:\n",
    "\n",
    "    if not solution_chunks:\n",
    "        return 0.0\n",
    "\n",
    "    student_emb = embedding_manager.generate_embeddings([student_answer])[0]\n",
    "\n",
    "    matched = 0\n",
    "    for chunk in solution_chunks:\n",
    "        chunk_emb = embedding_manager.generate_embeddings([chunk])[0]\n",
    "        sim = cosine_similarity([student_emb], [chunk_emb])[0][0]\n",
    "\n",
    "        if sim >= 0.65:   # semantic threshold\n",
    "            matched += 1\n",
    "\n",
    "    return matched / len(solution_chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c2042",
   "metadata": {},
   "source": [
    "NUMERIC RULE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c093481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_rule_score(student_answer: str, numeric_rules: dict) -> float:\n",
    "    if not numeric_rules:\n",
    "        return 0.0\n",
    "\n",
    "    expected_numbers = numeric_rules.get(\"expected_numbers\", [])\n",
    "\n",
    "    if not expected_numbers:\n",
    "        return 1.0   # nothing expected ‚Üí full marks\n",
    "\n",
    "    text = student_answer.lower()\n",
    "    matched = 0\n",
    "\n",
    "    for num in expected_numbers:\n",
    "        if str(num).lower() in text:\n",
    "            matched += 1\n",
    "\n",
    "    return matched / len(expected_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2956e50",
   "metadata": {},
   "source": [
    "SEMANTIC SIMILARITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60ee3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_score(\n",
    "    student_answer: str,\n",
    "    model_answer: str,\n",
    "    embedding_manager\n",
    ") -> float:\n",
    "\n",
    "    student_emb = embedding_manager.generate_embeddings([student_answer])\n",
    "    model_emb = embedding_manager.generate_embeddings([model_answer])\n",
    "\n",
    "    return cosine_similarity(student_emb, model_emb)[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab61252",
   "metadata": {},
   "source": [
    "GRADING WITH LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee5aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_technical_question(\n",
    "    student_answer: str,\n",
    "    technical_config: dict,\n",
    "    embedding_manager,\n",
    "    max_marks: int\n",
    ") -> int:\n",
    "\n",
    "    # 1. Semantic similarity\n",
    "    semantic_score = semantic_similarity_score(\n",
    "        student_answer,\n",
    "        technical_config[\"model_answer\"],\n",
    "        embedding_manager\n",
    "    )\n",
    "\n",
    "    # 2. Solution chunk coverage\n",
    "    chunk_score = solution_chunk_score(\n",
    "        student_answer,\n",
    "        technical_config.get(\"solution_chunks\", []),\n",
    "        embedding_manager\n",
    "    )\n",
    "\n",
    "    # 3. Keyword coverage\n",
    "    kw_score = keyword_score(\n",
    "        student_answer,\n",
    "        technical_config.get(\"keywords\", [])\n",
    "    )\n",
    "\n",
    "    # 4. Numeric / rule-based score\n",
    "    numeric_score = numeric_rule_score(\n",
    "        student_answer,\n",
    "        technical_config.get(\"numeric_rules\", {})\n",
    "    )\n",
    "\n",
    "    # Final weighted score\n",
    "    final_score = (\n",
    "        0.4 * semantic_score +\n",
    "        0.3 * chunk_score +\n",
    "        0.2 * kw_score +\n",
    "        0.1 * numeric_score\n",
    "    ) * max_marks\n",
    "\n",
    "    return round(final_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24ce5f",
   "metadata": {},
   "source": [
    "DESCRIPTIVE CHECKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab37d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_descriptive_question(\n",
    "    question: dict,\n",
    "    student_answer: str,\n",
    "    embedding_manager\n",
    ") -> dict:\n",
    "\n",
    "    print(\"Grading descriptive question...\")\n",
    "\n",
    "    max_marks = question[\"max_marks\"]\n",
    "\n",
    "    descriptive_cfg = question.get(\"descriptive_config\")\n",
    "    if descriptive_cfg is None:\n",
    "        print(\"‚ö†Ô∏è descriptive_config missing ‚Üí semantic fallback\")\n",
    "        marks = semantic_fallback(question, student_answer, embedding_manager)\n",
    "        return {\n",
    "            \"marks\": marks,\n",
    "            \"feedback\": \"Answer was evaluated using semantic similarity due to missing rubric.\"\n",
    "        }\n",
    "\n",
    "    rubric = descriptive_cfg.get(\"rubric\")\n",
    "    if not rubric:\n",
    "        print(\"‚ö†Ô∏è rubric missing ‚Üí semantic fallback\")\n",
    "        marks = semantic_fallback(question, student_answer, embedding_manager)\n",
    "        return {\n",
    "            \"marks\": marks,\n",
    "            \"feedback\": \"Rubric was unavailable, so semantic evaluation was applied.\"\n",
    "        }\n",
    "\n",
    "    if not student_answer.strip():\n",
    "        return {\n",
    "            \"marks\": 0,\n",
    "            \"feedback\": \"No answer was provided.\"\n",
    "        }\n",
    "\n",
    "    rubric_prompt = \"\"\n",
    "    trait_max_map = {}\n",
    "\n",
    "    for r in rubric:\n",
    "        trait = r[\"trait\"]\n",
    "        trait_marks = round(r[\"weight\"] * max_marks)\n",
    "        trait_max_map[trait] = trait_marks\n",
    "\n",
    "        rubric_prompt += f\"\"\"\n",
    "Trait: {trait}\n",
    "Max Marks: {trait_marks}\n",
    "Description: {r['description']}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an experienced university examiner.\n",
    "\n",
    "Evaluate the student answer STRICTLY using the rubric below.\n",
    "\n",
    "Question:\n",
    "{question['question_text']}\n",
    "\n",
    "Rubric (use ONLY these traits and max marks):\n",
    "{rubric_prompt}\n",
    "\n",
    "Student Answer:\n",
    "{student_answer}\n",
    "\n",
    "TASKS:\n",
    "1. Assign INTEGER marks for each trait.\n",
    "2. Provide SHORT constructive feedback for the student.\n",
    "\n",
    "SCORING RULES:\n",
    "- Assign INTEGER marks only.\n",
    "- Score each trait independently.\n",
    "- Use values from 0 up to Max Marks.\n",
    "- Partial credit is allowed.\n",
    "- Do NOT invent traits.\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON ONLY):\n",
    "{{\n",
    "  \"scores\": {{\n",
    "    \"<trait_name>\": <integer_marks>\n",
    "  }},\n",
    "  \"feedback\": \"<2‚Äì3 sentence constructive review>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=\"llama3:latest\",\n",
    "            prompt=prompt.strip()\n",
    "        )\n",
    "\n",
    "        data = json.loads(response[\"response\"])\n",
    "\n",
    "        scores = data.get(\"scores\", {})\n",
    "        feedback = data.get(\"feedback\", \"No feedback generated.\")\n",
    "\n",
    "        total = 0\n",
    "        for trait, max_trait_marks in trait_max_map.items():\n",
    "            awarded = int(scores.get(trait, 0))\n",
    "            awarded = max(0, min(awarded, max_trait_marks))\n",
    "            total += awarded\n",
    "\n",
    "        return {\n",
    "            \"marks\": min(total, max_marks),\n",
    "            \"feedback\": feedback\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è LLM failed:\", e)\n",
    "        marks = semantic_fallback(question, student_answer, embedding_manager)\n",
    "        return {\n",
    "            \"marks\": marks,\n",
    "            \"feedback\": \"Automatic feedback unavailable due to evaluation error.\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396c695",
   "metadata": {},
   "source": [
    "EVALUATING FROM THE FILE UPLOADED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8d5cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_student_answers(\n",
    "    exam_id: str,\n",
    "    answer_file_path: str,\n",
    "    embedding_manager\n",
    "):\n",
    "    final_results = []\n",
    "\n",
    "    answer_texts = extract_student_text(answer_file_path)\n",
    "    print(\"Extracted student answers:\", answer_texts)\n",
    "\n",
    "    for i, student_answer in enumerate(answer_texts):\n",
    "        question_number = i + 1\n",
    "\n",
    "        question = fetch_question(exam_id, question_number)\n",
    "        print(\n",
    "            f\"Q{question_number} question_type raw = {repr(question['question_type'])}\"\n",
    "        )\n",
    "\n",
    "        if question is None:\n",
    "            continue\n",
    "\n",
    "        marks = 0\n",
    "\n",
    "        if question[\"question_type\"] == \"TECHNICAL\":\n",
    "            tech = question[\"technical_config\"]\n",
    "            marks = grade_technical_question(\n",
    "                student_answer=student_answer,\n",
    "                technical_config=tech,\n",
    "                embedding_manager=embedding_manager,\n",
    "                max_marks=question[\"max_marks\"]\n",
    "            )\n",
    "\n",
    "        elif question[\"question_type\"] == \"DESCRIPTIVE\":\n",
    "            marks = grade_descriptive_question(\n",
    "                question,\n",
    "                student_answer,\n",
    "                embedding_manager\n",
    "            )\n",
    "\n",
    "        final_results.append({\n",
    "            \"question_number\": question_number,\n",
    "            \"marks_awarded\": marks,\n",
    "            \"max_marks\": question[\"max_marks\"]\n",
    "        })\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cdec1",
   "metadata": {},
   "source": [
    "API ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/evaluate\")\n",
    "def evaluate_exam(payload: EvaluateRequest):\n",
    "\n",
    "    exam_id = \"CS_ADV_2025\"\n",
    "    local_file_path = \"test_answer.pdf\"\n",
    "\n",
    "    try:\n",
    "        '''# Fetch submission (Cloudinary URL already in DB)\n",
    "        submission = fetch_submission(payload.submission_id)\n",
    "        if not submission:\n",
    "            raise HTTPException(status_code=404, detail=\"Submission not found\")\n",
    "\n",
    "        file_url = submission.get(\"answer_file_url\")\n",
    "        if not file_url:\n",
    "            raise HTTPException(status_code=400, detail=\"No file URL in submission\")\n",
    "\n",
    "        # Download student answer file\n",
    "        local_file_path = download_file_from_cloudinary(file_url)'''\n",
    "\n",
    "        # üî• CALL YOUR FUNCTION üî•\n",
    "        final_results = evaluate_student_answers(\n",
    "            exam_id=exam_id,\n",
    "            answer_file_path=local_file_path,\n",
    "            embedding_manager=embedding_manager\n",
    "        )\n",
    "\n",
    "        print(\"Final Results:\", final_results)\n",
    "\n",
    "        '''return {\n",
    "            \"exam_id\": exam_id,\n",
    "            \"submission_id\": payload.submission_id,\n",
    "            \"results\": final_results\n",
    "        }'''\n",
    "\n",
    "    finally:\n",
    "        if local_file_path and os.path.exists(local_file_path):\n",
    "            os.remove(local_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
